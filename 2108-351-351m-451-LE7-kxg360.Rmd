---
output:
  pdf_document: default
  html_document: default
---
                                                                              ---                   
title: 'CWRU DSCI351-351m-451: Lab Exercise LE7 NAME'
subtitle: 'Inference, Linear Regression, Timeseries Analysis'
author: "Prof.:Roger French, TA: Raymond Wieser, Sameera Nalin Venkat"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: TRUE
    number_sections: TRUE
    toc_depth: 6
    highlight: tango
  html_notebook:
  html_document:
    css: ../lab.css
    highlight: pygments
    theme: cerulean
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: paged
urlcolor: blue
always_allow_html: true
---

\setcounter{section}{7}
\setcounter{subsection}{0}

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
  fig.width = 6, # the width for plots created by code chunk
  fig.height = 4, # the height for plots created by code chunk
  fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
  dpi = 300, 
  dev = 'png', # Makes each fig a png, and avoids plotting every data point
  # eval = FALSE, # if FALSE, then the R code chunks are not evaluated
  # results = 'asis', # knitr passes through results without reformatting
  echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
  message = TRUE, # if FALSE knitr won't display messages generated by code
  strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
  warning = FALSE, # if FALSE knitr won't display warning messages in the doc
  error = TRUE) # report errors
  # options(tinytex.verbose = TRUE)
```

### LE7, 10 points,  questions. 

  - Q1 - OIS: Numerical inference, 1 pt.
  - Q2 - OIS: Linear regression, 1 pt.
  - Q3 - OIS: Logistic regression, 1 pt.
  - Q4 - Logistic regression: Palmer's penguins, 3 pts.
  - Q5 - Houston crime data, 4 pts.

#### Lab Exercise (LE) 7

```{r}

set.seed(1)
library(tidyverse)
```

--------------------------------------------

## Q1. OIS: Numerical inference (1 point)

OIS v3 5.44: Teaching descriptive statistics.

A study compared five different methods for teaching descriptive statistics. 

  - The five methods were 
    - traditional lecture and discussion, 
    - programmed textbook instruction, 
    - programmed text with lectures, 
    - computer instruction, 
    - and computer instruction with lectures. 
  - 45 students were randomly assigned, 
    - 9 to each method. 
  - After completing the course, 
    - students took a 1-hour exam.

What are the hypotheses for evaluating 
  - if the average test scores are different 
    - for the different teaching methods?

What are the degrees of freedom associated with the F -test 

  - for evaluating these hypotheses?

Suppose the p-value for this test is 0.0168. 

  - What is the conclusion?


```{r}

### Hypothesis
# 
# H0: The mean score is the same across all groups. 
# HA: At least one mean score is different.

# Generally we must check three conditions on the data before performing ANOVA:
# • the observations are independent within and across groups,
# • the data within each group are nearly normal, and
# • the variability across the groups is about equal.

# There are two degree of freedoms associated with these tests
# mean square between groups (MSG), and it has an associated
# degrees of freedom,dfG k-1, when there are k groups

k= 5

dFg =  k -1

# We need a benchmark value for how much variability should be expected among the sample means if the null hypothesis is true. To this end, we compute a pooled variance estimate, often abbreviated as the mean square error (MSE), which has an associated degrees of freedom value dfE = n - k.
n = 45

dFE = n - k


```

ANSWER:

Hypothesis
 
H0: The mean score is the same across all groups. 
HA: At least one mean score is different.

There are two degree of freedoms associated with these tests:

Mean square between groups (MSG), and it has an associated
degrees of freedom,dfG =  k-1, when there are k groups.

dFg : 4


We need a benchmark value for how much variability should be expected among the sample means if the null hypothesis is true. To this end, we compute a pooled variance estimate, often abbreviated as the mean square error (MSE), which has an associated degrees of freedom value dfE = n - k.

dFE : 40


The p-value is smaller than 0.05, indicating the evidence is strong enough to reject the null hypothesis
at a significance level of 0.05.




--------------------------------------------

## Q2. OIS: Linear regression   (1 point)

OIS v3 7.12: Trees. 

This dataset shows the relationship between height, diameter, and volume of timber in 31 felled black cherry trees. The diameter of the tree is measured 4.5 feet above the ground.

Visualize the relationships in the data

  - height vs. volume
  - diameter vs. volume
  - your visualizations should contain all important info (labels, etc.)
  - (hint: scatterplots)
  
Let's answer questions using linear regression

  - Describe the relationship between volume and height of these trees.
  - Describe the relationship between volume and diameter of these trees.
  - The summarizing the model results from the lm() function will provide valuable numerical insights.

Suppose you have height and diameter measurements for another black cherry tree. Which of these variables would be preferable to use to predict the volume of timber in this tree using a simple linear regression model? Explain your reasoning.




```{r}
library(tidyverse)
datasets::trees

tree_dt <- as.data.frame(datasets::trees)

# visualizing the data with scatterplots
#- height vs. volume
tree_dt  %>%
  ggplot(aes(x = Height , y = Volume)) + geom_point()

cor(tree_dt$Volume, tree_dt$Height)

#.- diameter vs. volume
tree_dt  %>%
  ggplot(aes(x = Girth , y = Volume)) + geom_point()  

cor(tree_dt$Volume, tree_dt$Girth)

# Let's answer questions using linear regression

  # - Describe the relationship between volume and height of these trees.
  # - Describe the relationship between volume and diameter of these trees.
  # - The summarizing the model results from the lm() function will provide valuable numerical insights.
# analyzing the data with lm()

mdla <- lm(Volume ~ Height, tree_dt)
mdlb <- lm(Volume ~ Girth, tree_dt)

summary(mdla)
summary(mdlb)

```

ANSWER: -
<!-- - Describe the relationship between volume and height of these trees. -->
<!-- - Describe the relationship between volume and diameter of these trees. -->

- Direction : Both Height and Girth have a positive direction relationhip:
meaning a positive change in either of these variables gives a positive relationship
in Volume of tree

- Form : Both have a linear relationship with volume

- Strength :  correlation : 0.5982497 for Height
              correlation : 0.9671194 for Girth

- Girth is significanty correlated with Volume

- Looking at the summary of model statistics, 

- Adjusted R-squared:  0.3358   p-value -0.000378 ***
- Adjusted R-squared:  0.9331   p-value: < 2.2e-16 ***

- Girth explains 0.9331 of the variability of the volume of tree
- and has much smaller  p - value


<!-- Suppose you have height and diameter measurements for another black cherry tree. Which of these variables would be preferable to use to predict the volume of timber in this tree using a simple linear regression model? Explain your reasoning. -->

- Because of these above reasons,  i will prefer to use Girth to predict Volume of
- another set of cherry trees based on linear regression model.



--------------------------------------------

## Q3. OIS: Logistic regression  (1 point)

OIS v3 8.16 Challenger disaster, Part I. 

On January 28, 1986, a routine launch was anticipated for the Challenger space shuttle. Seventy-three seconds into the flight, disaster happened: the shuttle broke apart, killing all seven crew members on board. An investigation into the cause of the disaster focused on a critical seal called an O-ring, and it is believed that damage to these O-rings during a shuttle launch may be related to the ambient temperature during the launch.

The orings.txt file in the data folder contains data on the temperature and number of damaged O-rings for 23 shuttle missions, where the mission order is based on the temperature at the time of the launch. Temp gives the temperature in Fahrenheit, Damaged represents the number of damaged O-rings. There are 6 O-rings total, so the number of undamaged O-rings can be calculated.

Visualize the data. 

  - what relationships do you observe between temperature and failure?

Create a logistic regression model. 

  - classify each case as having either damaged or undamaged O-rings (1 or 0)
    - a binary "failure" variable will help us determine probability of failure as a result
  - use temperature as a predictor
  - use the glm() function 
  - display the summary statistics of your model

Based on the model, do you think concerns regarding O-rings are justified? Explain.

  - what does the p-value tell you? 
  
What assumption has to be made for logistic regression to hold valid in this case?


```{r}
read.table("data/orings.txt", header = TRUE)

oring <- read.table("data/orings.txt", header = TRUE)



# initial EDA
# Visualize the data.
# 
#   - what relationships do you observe between temperature and failure?

# In general, lower temp is associated with damaged O-rings but at higher temp they
# all seem to be undamaged.
ggplot(oring, aes(x = temp, y = damage)) + geom_point()

# logistic regression model

??glm()


## create failure column (binary, 1 or 0)
oring <- oring %>%
  mutate(failure = if_else(damage > 0, 1 ,0))


## use and summarize the glm() function

mdl <- glm(failure ~ temp,  data = oring, family = "binomial")
summary(mdl)
```
ANSWER:

<!-- Visualize the data.  -->

<!--   - what relationships do you observe between temperature and failure? -->
 In general, lower temp is associated with damaged O-rings but at higher temp they  all seem to be undamaged.
 
 <!-- Based on the model, do you think concerns regarding O-rings are justified? Explain. -->

 <!--  - what does the p-value tell you? --  -->
 
Statistically significant:  0.0320 * p- value is less than 0.05
There is some relationship between temp and faiure rate.
 
<!-- What assumption has to be made for logistic regression to hold valid in this case?  -->
  Independence of errors, linearity in the logit for continuous variables, absence of multicollinearity, and lack of strongly influential outliers.

--------------------------------------------

## Q4. Logistic regression: Palmer's penguins (3 points)

Let's make some logistic models on Palmer's penguins.

  - we've looked at regression with a single predictor
  - let's make a logistic model with multiple predictors
    - we're increasing the dimensions of the model in order to get more information out of the data
  - we want to create a model that can predict penguin species
  
We will used the package nnet

  - This package is used for machine learning
  - But it also has a function
    - nnet::multinom()
      - Which works like stats::lm()
      - see ?nnet::multinom() for more help
      
  - This is because logistic models
    - Are used to baseline more complex Machine Learning Models
      - For performance
      
  - nnet::multinom() is used to build multiple logistic models
    - Which can be used to classify multiple outputs
    - Instead of a binary like a logistic model

```{r}
library(nnet)
library(caret)
glimpse(palmerpenguins::penguins)


```


### Q4.1 Setup Training & Testing Dataframes

Processing the data

  - divide the data into training and testing datasets
    - Using caret::createDataPartiton()
    - Which will divide the data into groups
      - So we can train the model on one subset
      - And use the other subset to test the models accuracy

```{r}
# Get the partition
# Gives a list of indices that equally represent the column,on which it is partitioned 

df <- palmerpenguins::penguins
idx <- createDataPartition(df$species, p = 0.8, list = FALSE, 
                                  )
#Subset the data
#Create training and testing dfs
training <- df[idx, ] # use the indices to obtain the ful rows
testing <- anti_join(df, training) # get the leftover rows

```
ANSWER: Divided the main dataset into training and testing, with 80% in training and 20 % testing.


### Q4.2 Build a logistic regression model

Build a logistic model 

  - to predict species
  - Based on all the predictors in the dataset
    - The short hand way of doing this is to use 
    - model_function_call(Response_Variable ~ .)
    - Where the . will automatically use the rest of the columns
      - As predictors
      
  - Once you have obtained your model object
  - It's time to use stats::predict()
    - Which takes in a model object
    - Then applies it to new data (a testing subset)
    
  - Different model types can have different prediction classes
    - Here we are trying to predict class

```{r}
#Build your model

fit <- multinom(species ~ ., data = training)

## what does binomial do

summary(fit)

#Predict classes
predict (fit, testing) 


```

ANSWER:

### Q4.3 Evaluate accuracy on your test data

Evaluate the accuracy of your model against test data

  - create a confusion matrix to evaluate your results
  - using caret::confusionMatrix()
    - This compares the predicted class against the actual class
    - Which shows how the model classified the data

Are there any things that were commonly misclassified? 

  - Why do you think the model had trouble with these?
  - What can be done to improve this model? 

```{r}
#Create Confusion Matrix
#From prediction and observed data


confusionMatrix(predict (fit, testing), testing$species)



```

ANSWER:

<!-- Are there any things that were commonly misclassified?  -->

<!--   - Why do you think the model had trouble with these? -->
<!--   - What can be done to improve this model?  -->

- Prediction  Adelie Chinstrap Gentoo
- Adelie        29         2      0
- Chinstrap      0        11      0
- Gentoo         0         0     22

There are 2 Chinstraps that were misclassified as Adelie.
Since, I know that year is an uneccessary variable ansd cannot  possibly determine the species, i remove it to get a accuarcy of 100 %

- Prediction  Adelie Chinstrap Gentoo
-  Adelie        29         0      0
-  Chinstrap      0        13      0
-  Gentoo         0         0     22

```{r}

#Build your model

fit2 <- multinom(species ~ island  + bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + sex, data = training)

## what does binomial do

summary(fit2)

#Predict classes
predict (fit2, testing) 

confusionMatrix(predict (fit2, testing), testing$species)



```

--------------------------------------------

## Q5 Houston Crime Reports

We will be working with the Huston crime data file provided by the ggmap package, ggmap::crimes.

- This CSV file contains the location (latitude and longitude)
- For crimes reported
- From January 2010 - August 2010

```{r}
library(ggmap)
library(sp)
library(rgdal)
library(leaflet)
library(lubridate)
library(RColorBrewer)
library(classInt)
library(tidyverse)
```


### Q5.1 EDA to identify trends

Exploratory Data Analysis (EDA)

Let's do some exploratory data analysis on the data

  - we'll start with the temporal aspect of the crime data.
  - what can we say about **when** people commit crimes?
  
What trends do you see looking at different time frames?

  - what months have particularly high crime rates?
  - what times of day have increased crime rates?
  - what days of the week have higher crime rates?
  - produce three different visuals that represent each of these trends.

(hint: histograms are helpful for showing distributions)

Which of these trends could you have predicted? Does anything surprise you?

Are there any relationships between types of crime and time of day?

  - produce a stacked histogram and comment on the results

```{r}
# Load Data
t <- theme(legend.text = element_text(size = 4) ) +
theme(legend.key.size = unit(0.5, 'cm') ,
axis.text.x = element_text(size = 6,
face = "bold", angle = 45, hjust = 1))

Houston_Crime_Reports <- ggmap::crime

#Crime per Day
# what days of the week have higher crime rates?
Houston_Crime_Reports <- Houston_Crime_Reports %>% 
separate(time, c("date_new", "time_new"), " ", extra = "merge")%>%
mutate(day_week = wday(date_new, label=T) )

Houston_Crime_Reports %>%
  ggplot(aes(x = day, fill = offense)) + geom_bar(stat = "count") + t

## why are there so many Nas
na_fil <-Houston_Crime_Reports %>%
  filter(is.na(day))


#Crime Per Month
# - what months have particularly high crimes rates?
Houston_Crime_Reports %>%
  ggplot(aes(x = month, fill = offense)) + geom_bar(stat = "count") + t


#Crime Per Hour
Houston_Crime_Reports %>%
mutate(time_new = as.POSIXct(hms::parse_hm(time_new))) %>%
ggplot(aes(x = time_new)  ) +
geom_bar() +
scale_x_datetime(date_labels = "%H:%M") + t + facet_wrap(~ offense)

#Crime Per Hour
Houston_Crime_Reports %>%
mutate(time_new = as.POSIXct(hms::parse_hm(time_new))) %>%
  ggplot(aes(x = time_new , fill = offense)  ) +
geom_bar() +
scale_x_datetime(date_labels = "%H:%M") + t 

```

ANSWER: The highest number of crimes happen on Friday, while the lowest number is
on Sunday.
The highest number of crimes happen in may , while the lowest happen in february.
Crimes reduce as the morning approaches and are at the least between 5 to 7am,
In the afternoon there is  peak at 1 pm , there is peak in the evening at 7pm, when the
highest number of crimes are reported. Crimes again peak as pitch black night approaches,
from 11 pm to 1am.

Looking at the stacked histograms

Across the week, types of crime and their variations remains constant, with theft,
burglary being the highest and eveything else varying equally and murder being 
the lowest.

Across the year, types of crime and their variations remains constant, with theft,
burglary being the highest and eveything else varying equally and murder being 
the lowest.

Across a day, 
rapes happen only at night,robberies happen mostly at night,
burglaries seem to peak 8 to 10.
Theft peaks at 1pm in the afternoon and happens all the night hours
Agravated assault minimises from 6 to 8 am

I could have predicted that rapes happen only during the night hours.
Sundays are low crime days and Fridys are high crime days.

I could not have predicted the months of the highest and lowest crimes.   


### Q5.2 Geospatial Analysis

```{r}
library(sp)
library(rgdal)
library(maptools)
library(broom)
```

Geospatial Analysis

Next we'll look at the spatial distribution of this data.

  - Plot the data on a OpenStreetMap
    - Using source = "stamen"
  - You will have to specify the location in the function call
      - This is because ggmap::get_map()
      - Defaults to Google Maps when a bounding box is not specified
      - A bounding box 
        - Gives the boundaries of the map that is downloaded
        - Specified with a list
        - c(left = '' , right = '' , top = '', bottom = '')
  - Color the map based on 
    - Type of Crime Reported
  

```{r}

#Get the bbox

bbox = c(left = -96.1002 , bottom = 29.4934, right = -94.8230, top = 30.1546)


#Retrieve the map

houston_map <- get_stamenmap(
bbox,
maptype = "toner-lite",
zoom = 10
)


#Plot

rep1 <- ggmap(houston_map) + 
  geom_point(data = Houston_Crime_Reports, aes(x = lon, y = lat, color = offense))

rep2 <- ggmap(houston_map) + 
  geom_point(data = Houston_Crime_Reports, aes(x = lon, y = lat) ) + facet_wrap( ~ offense)




rep1
rep2

```

ANSWER: 

### Q5.3a Modify the incident occurrence layer, to better see whats happening

In the last map, it was a bit tricky 

  - to see the density of the incidents 
    - because all the graphed points 
    - were sitting on top of each other.  

We're going to now modify the incident occurrence layer 

  - to plot the density of points 
    - vs plotting each incident individually.  
  - We accomplish this with 
    - the ggplot2::stat_density2d() function vs ggplot2::geom_point().
  

```{r}
#Plot using stat_density2d()

??stat_density2d()



ggmap(houston_map) + 
geom_point( data = Houston_Crime_Reports, aes(x = lon, y = lat ), 
            color = "red", size =0.01)  +
stat_density2d(
aes(x = lon, y = lat , fill = ..level..),
size = 0.02, bins = 15, data = Houston_Crime_Reports,
geom = "polygon"
) + scale_fill_gradientn(colours=rev(brewer.pal(7,"Spectral")))+ 
  labs(title = "Density plot with actual points in red")

ggmap(houston_map)  +
stat_density2d(
aes(x = lon, y = lat , fill = ..level..),
size = 0.02, bins = 15, data = Houston_Crime_Reports,
geom = "polygon"
) +   scale_fill_gradientn(colours=rev(brewer.pal(7,"Spectral")))+ 
facet_wrap(~offense) + labs(title = "Density plot showing different crimes")





```

ANSWER:

### Q5.3b  What does ..level.. do

  - What does ..level.. do
    - in the ggplot2::stat_density2d() function call?
    - Hint: Look at the help topics for this function

ANSWER: ..level.. is a parameter that is assigned to a point based on the number 
of data points in the neighborhood of that point, in a way a number that denotes
density, higher the ..level.. more the data points around that neighborhood.


### Q5.4 What is the safest and most dangerous neighborhoods

Finally 

  - Filter out a specific crime of your choosing
  - Plot the crime density
  
We will use a new package that assists in geospatial analysis

  - rdgal
  - This package is used to transform and project geospatial objects
  - It also has some nice functions for working with .shp files
    - .shp files contain information about regions on a map
    - i.e .shp files can contain the information
      - the size, shape, and location of countries or states

  - Add Polygons for the specific Neighborhoods
    - Using NeighborShapefile
      - and rdgal::readOGR()
      - or mapdata::readShapeSpatial()
      
  - What is the most dangerous Neighborhood for your crime
  - Where is the safest neighborhood?
    
  - Label the map with the neighborhoods
    - Hint: It's OK to remove some of the labels, there's a lot
      - ggplot2::geom_text() has a built in function for this
      - check overlap = TRUE


  

```{r}



# Filter
# filter a particular crime  - burglaries
burglary_dt <- Houston_Crime_Reports %>%
  filter(offense == "burglary")




#Read Shapefile
texas_shp <- readOGR(dsn = "data/shp", 
                     layer = "COH_SUPER_NEIGHBORHOODS")


#Find the location of the center of the neighborhood
 
cent  <- coordinates(texas_shp)
county <- texas_shp@data$SNBNAME
coord_tex <- cbind( coordinates(texas_shp), as.character(texas_shp@data$SNBNAME))
coord_tex <-as.data.frame(coord_tex)
head(coord_tex)
                           

coords <- SpatialPoints(burglary_dt[, c("lon", "lat")])
crime_spatial_df <- SpatialPointsDataFrame(coords, burglary_dt)
proj4string(crime_spatial_df) <- CRS("+proj=longlat +ellps=WGS84")

crime_df <- data.frame(crime_spatial_df)
texas_shp_df <- fortify(texas_shp)



#Get the neigborhood names
texas_shp@data[["SNBNAME"]]



# data_prep
coord_tex <- rename(coord_tex, lon = V1, lat = V2 , county = V3)

coord_tex$lon <- as.numeric(as.character(coord_tex$lon))
coord_tex$lat <- as.numeric(as.character(coord_tex$lat))
coord_tex$county <- as.character(coord_tex$county)

crime_df_new <- right_join(crime_df, coord_tex )



#Plot with density + neighborhood + label
stat <-  ggmap(houston_map)  +
stat_density2d(aes(x = lon, y = lat , fill = ..level.. ),
size = 0.02, bins = 15, data = crime_df, geom = "polygon") +   scale_fill_gradientn(colours=rev(brewer.pal(7,"Spectral"))) + 
  labs(title = "Density plot") 
  
  
# county <- ggmap(unitedStatesmap) +  geom_polygon(aes( x = long, y = lat, group = group, label = as.character(id)), data = texas_shp_df, colour = "black",
# alpha = .4, size = .3)   


k <- stat + geom_polygon(aes( x = long, y = lat, group = group),
                         data = texas_shp_df, colour = "black",
alpha = .4, size = .3) 

k + geom_text(data = crime_df_new,aes(x = lon, y = lat, label = county) ,
              check_overlap = TRUE, size= 2)



```

ANSWER:  Based on an rudimentary look at the plot obtained.

The most dangerous neighborhood for burglaries :

The most safest neighborhood for burglaries : SHARPSTOWN ,	
ALIEF

There are several neighbourhoods in the edge of houston , marked in blue that
are the safest


## Links

[https://blog.dominodatalab.com/applied-spatial-data-science-with-r/](https://blog.dominodatalab.com/applied-spatial-data-science-with-r/)

[http://www.r-project.org](http://www.r-project.org) 

[http://rmarkdown.rstudio.com/](http://rmarkdown.rstudio.com/)  

[https://www.openintro.org/stat/textbook.php?stat_book=os](https://www.openintro.org/stat/textbook.php?stat_book=os)

[https://en.wikipedia.org/wiki/Multivariate_kernel_density_estimation](https://en.wikipedia.org/wiki/Multivariate_kernel_density_estimation)