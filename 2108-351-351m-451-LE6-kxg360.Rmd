---
title: 'CWRU DSCI351-351m-451: Lab Exercise LE6 NAME'
subtitle: 'Inference, Linear Regression, Timeseries Analysis'
author: "Prof.:Roger French, TA: Raymond Wieser, Sameera Nalin Venkat "
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: TRUE
    number_sections: TRUE
    toc_depth: 6
    highlight: tango
  html_notebook:
  html_document:
    css: ../lab.css
    highlight: pygments
    theme: cerulean
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: paged
urlcolor: blue
always_allow_html: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
  fig.width = 5, # the width for plots created by code chunk
  fig.height = 3, # the height for plots created by code chunk
  fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
  dpi = 300, 
  dev = 'png', # Makes each fig a png, and avoids plotting every data point
  # eval = FALSE, # if FALSE, then the R code chunks are not evaluated
  # results = 'asis', # knitr passes through results without reformatting
  echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
  message = TRUE, # if FALSE knitr won't display messages generated by code
  strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
  warning = FALSE, # if FALSE knitr won't display warning messages in the doc
  error = TRUE) # report errors
  # options(tinytex.verbose = TRUE)
```

\setcounter{section}{6}
\setcounter{subsection}{0}
```{r}
## loading libraries

library(ggplot2)
library(GGally)
library(glue)
library(dplyr)

```


### LE6, 10 points, 4 questions. 

  - LE6.1, 1 pt.
  - LE6.2, 2 pts.
  - LE6.3, 4 pts.
  - LE6.4, 3 pts.

#### Lab Exercise (LE) 6

- Inference Guide

There is a useful Inference Cheat Sheet 

  - in your 3-readings/3-CheatSheets/ folder
  - `os2_extra_inference_guide.pdf`

-----

## LE6.1. Fuel efficiency of Prius. (OIS 5.8)

- [Fueleconomy.gov](http://www.fueleconomy.gov/), 

  - the official US government source 
    - for fuel economy information, 
  - allows users to share gas mileage information on their vehicles. 
  
The histogram below shows 

  - the distribution of gas mileage in miles per gallon (MPG) 
    -  from 14 users who drive a 2012 Toyota Prius. 
  - The sample mean is 53.3 MPG 
    - and the standard deviation is 5.2 MPG. 
  
Note that these data are user estimates 

  - and since the source data cannot be verified, 
  - the accuracy of these estimates are not guaranteed.[@noauthor_gas_nodate]

![Mileage in MPG](./figs/5-08-1.png)

### LE6.1.1 We would like to use these data to evaluate 

  - the average gas mileage of all 2012 Prius drivers.

Do you think this is reasonable? 

  - Why or why not? 

#### LE6.1.1 Answer:

The central limit theorem states that if you have a population with mean μ and 
standard deviation σ and take sufficiently large random samples from the 
population with replacement , then the distribution of 
the sample means will be approximately normally distributed. This will hold
true regardless of whether the source population is normal or skewed, 
provided the sample size is sufficiently large (usually n > 30). 
If the population is normal, then the theorem holds true even for samples
smaller than 30.

We have here, 14 observations, but they are normally distributed, therefore,
we can use them to find the true mean gas mileage for all Prius users.



### LE6.1.2 The EPA claims that a 2012 Prius gets 50 MPG 

  - (city and highway mileage combined). 
  
Do these data provide strong evidence against this estimate 

  - for drivers who participate on http://fueleconomy.gov? 
  - Note any assumptions you must make as you proceed with the test.

#### LE6.1.2 Answer:

This data does not provide enough evidence against this estimate.
There is a good possibility that true mean is 50.

sample mean : 53.3
null value : 50

Hypothesis : 
- NULL : The true mean is same as what is proposed : 50 
- ALTERNATE: The true mean is not 50, but something else.

significance level  = 0.05

```{r}

sample_mean = 53.3
proposed_value = 50 ## given in question
n = 14
significance_level = 5/100
std_deviation = 5.2

point_estimate = sample_mean
null_value = proposed_value
standard_error = std_deviation / sqrt(n)


z_score = (point_estimate - null_value)/standard_error
p_value = 2 * pnorm(z_score, lower.tail = FALSE)

## 2- sided
# In an upper-tailed test the decision rule has investigators reject H0 if the
# test statistic is larger than the critical value. In a lower-tailed test the
# decision rule has investigators reject H0 if the test

if(p_value > significance_level){
glue("The chance of occurence of {point_estimate} is greater than
{significance_level} = > there is something going on and we reject H0")
}else
glue("The chance of occurence of {point_estimate} is lower than
{significance_level} = > point estimate came due to sampling
variability. There is nothing going on and we reject H1")


```



### LE6.1.3 Calculate a 95% confidence interval 

  - for the average gas mileage of a 2012 Prius 
  - by drivers who participate on fueleconomy.gov.
  
```{r}
critical_val_upper = point_estimate + 1.96 * standard_error
critical_val_lower = point_estimate - 1.96 * standard_error
```
  

#### LE6.1.3 Answer:

95% confidence intervel  (50.57 -> 56.023)

-----

## LE6.2. Diamonds 

### LE6.2.1 Diamonds Part I. (OIS 5.28)

- Prices of diamonds are determined by what is known as the 4 Cs:

  - cut,
  - clarity, 
  - color, 
  - and carat weight. 
  
The prices of diamonds go up 

  - as the carat weight increases, 
  -  but the increase is not smooth. 
  
For example, the difference between the size 

  - of a 0.99 carat diamond and 
    - a 1 carat diamond is undetectable to the naked human eye, 
  - but the price of a 1 carat diamond tends to be much higher 
    - than the price of a 0.99 diamond. 
  
In this question we use two random samples of diamonds, 

  - 0.99 carats and 1 carat, 
  - each sample of size 23, 
  
and compare the average prices of the diamonds. 

In order to be able to compare equivalent units, 

  - we first divide the price for each diamond 
  - by 100 times its weight in carats. 
  
That is, for a 0.99 carat diamond, we divide the price by 99. 

For a 1 carat diamond, we divide the price by 100. 

The distributions and some sample statistics are shown below.[@wickham_ggplot2:_2016]

![Point Price](./figs/5-28-1.png)

![Sample Statistics](./figs/5-28-2.png)

Conduct a hypothesis test to evaluate 

  - if there is a difference between the average standardized prices 
  - of 0.99 and  1 carat diamonds. 

Make sure to 

  - state your hypotheses clearly, 
  -  check relevant conditions, 
  - and interpret your results in context of the data.
 
#### LE6.2.1 Answer:
Hypothesis :

- NULL : There is no difference between the standardised prices of both these types
- ALTERNATE : There is a difference.

There is nothing going on, and we support the null hypothesis, the difference 
between the two prices is negligible

```{r}

sample_mean1 = 44.51
sample_mean2 = 56.81
std_deviation1 = 13.32
std_deviation2 = 16.13
proposed_value = 0 ## given in question
n = 23
significance_level = 5/100

point_estimate = sample_mean1 - sample_mean2
null_value = proposed_value
standard_error = sqrt( std_deviation1^2 / n + std_deviation1^2 / n)


z_score = (point_estimate - null_value)/standard_error
p_value = 2 * pnorm(z_score, lower.tail = TRUE)

## 2- sided
# In an upper-tailed test the decision rule has investigators reject H0 if the
# test statistic is larger than the critical value. In a lower-tailed test the
# decision rule has investigators reject H0 if the test

if(p_value > significance_level){
glue("The chance of occurence of {point_estimate} is greater than
{significance_level} = > there is something going on and we reject H0")
}else
glue("The chance of occurence of {point_estimate} is lower than
{significance_level} = > point estimate came due to sampling
variability. There is nothing going on and we reject H1")


 
 
```

### LE6.2.2 Diamonds Part II. (OIS 5.30)

- We discussed diamond prices

  - (standardized by weight) 
  -  for diamonds with weights 0.99 carats and 1 carat. 
  
See the table for summary statistics,

  - and then construct a 95% confidence interval 
  - for the average difference 
    - between the standardized prices of 0.99 and 1 carat diamonds. 

You may assume the conditions for inference are met.

#### LE6.2.2 Answer: 

Assuming the conditions for inference is met:



95% confidence intervel : (-19.998 : -4.6014)

```{r}
critical_val_upper = point_estimate + 1.96 * standard_error
critical_val_lower = point_estimate - 1.96 * standard_error
```




-----

## LE6.3 OIStats: Linear regression
 
- The movie Moneyball focuses on the “quest for the secret of success in baseball”. 

  - It follows a low-budget team, the Oakland Athletics, 
    - who believed that underused statistics, such as 
      - a player’s ability to get on base, 
    - better predict the ability to score runs than typical statistics 
      - like home runs, RBIs (runs batted in), and batting average. 
  - Obtaining players who excelled in these underused statistics 
    - turned out to be much more affordable for the team.

In this lab we’ll be looking at data from all 30 Major League Baseball teams 

  - and examining the linear relationship between runs scored in a season 
  - and a number of other player statistics. 

Our aim will be to summarize these relationships 

  - both graphically and numerically in order to find which variable, if any, 
  - helps us best predict a team’s runs scored in a season.

#### Data
 
- Let’s load up the data for the 2011 season.

  - Its a file `mlb11.RData`
  - Its located in the data folder of this LE6 in your class repo.
 
```{r}
load("./data/mlb11.RData")
```

In addition to runs scored, 

  - there are seven traditionally used variables in the data set: 
    - at-bats, 
    - hits, 
    - home runs, 
    - batting average, 
    - strikeouts, 
    - stolen bases, 
    - and wins. 

There are also three newer variables: 

  - on-base percentage, 
  - slugging percentage, 
  - and on-base plus slugging. 

For the first portion of the analysis 
  - we’ll consider the seven traditional variables. 
  - At the end of the lab, you’ll work with the newer variables.

### LE6.3.1

- What type of plot would you use to display 
 
  - the relationship between runs and one of the other numerical variables? 

Plot this relationship using the variable at_bats as the predictor. 

  - Does the relationship look linear? 

```{r}

## plot runs scored vs at_bats

# m1b11 %>%
#   ggplot(aes(x = ,y =))

## I would use the simple geom_point dot plot, to see any relationship

mlb11 %>%
  ggplot(aes(x = at_bats, y = runs)) + geom_point()


```

#### LE6.3.1a ANSWER:

I use the simple dot plot to observe the point as is.
Until 5600 at_bats, the runs increase linearly with at_bats, Therefore, it seems 
that there is a linear relationship.

- If you knew a team’s at_bats, 

 - would you be comfortable using a linear model 
 - to predict the number of runs?

Quantify the strength of the relationship with the correlation coefficient.

```{r}

## Correlation coefficient is found in pairwise plots

ggpairs(mlb11 %>% select(runs, at_bats))

cor(mlb11$runs, mlb11$at_bats)


## correlation 0.611

```

#### LE6.3.1b ANSWER: 

correlation coefficient : 0.61, Which means that 61% of the time,when at_bats
changes, runs would have a corresponding change.


##### Sum of squared residuals

- Think back to the way that we described the distribution of a single variable. 

  - Recall that we discussed characteristics such as center, spread, and shape. 
  - It’s also useful to be able to describe 
    - the relationship of two numerical variables, 
    - such as runs and at_bats above.

### LE6.3.2

- Looking at your plot from the previous exercise, 

  - describe the relationship between these two variables. 

Make sure to discuss 

  - the form, direction, and strength of the relationship 
  - as well as any unusual observations.

#### LE6.3.2 ANSWER: 

  - Form : Linear
  - Direction : Positive
  - Strength : correlation coefficient :0.61
  
  - runs has a linear relationship with at_bats
  - when at_bats has a positive change, the runs has a posiive change and 
  same goes for negative
  - atbats and runs have a correlation coefficient of 0.61
  
  
  


### LE6.3.3

- Just as we used the mean and standard deviation to summarize a single variable, 

  - we can summarize the relationship between these two variables 
    - by finding the line that best follows their association. 

Use the following interactive function 

  - to select the line that you think does the best job 
    - of going through the cloud of points.

```{r}
library(statsr)
plot_ss(x = mlb11$at_bats, y = mlb11$runs)
```

**You'll need to run this command in an R script (.R file)**

  - You don't get the interactive prompts when in a .Rmd file
  
After running this command in a .R script, you’ll be prompted 

  - to click two points on the plot to define a line. 

Once you’ve done that, 

  - the line you specified will be shown in black 
  - and the residuals in blue. 

Note that there are 30 residuals, one for each of the 30 observations. 

  - Recall that the residuals are the difference 
  - between the observed values and the values predicted by the line:

   $$e_i = y_i- \hat{y}_i$$    

The most common way to do linear regression 

  - is to select the line that minimizes the sum of squared residuals.

To visualize the squared residuals, 

  - you can rerun the plot command 
  - and add the argument `showSquares = TRUE`.

```{r}
plot_ss(x = mlb11$at_bats, y = mlb11$runs, showSquares = TRUE)
```

Note that the output from the `plot_ss` function 

  - provides you with the slope and intercept of your line 
  - as well as the sum of squares.

Using `plot_ss`, choose a line 

  - that does a good job of minimizing the sum of squares. 

Run the function several times. 

  - What was the smallest sum of squares that you got? 
  - How does it compare to your neighbors?

```{r}

## We have to run it many times and observe sum of squares function.
# Sum of Squares:  145914.4
# Sum of Squares:  144974.3
# Sum of Squares:  163010.4
# Sum of Squares:  125105.3
# Sum of Squares:  236679.7
# Sum of Squares:  175150.1

```

#### LE6.3.3a ANSWER: 
The smallest sum of squares  i got is 125105.3.
I could not comapare with a neighbor.

#####  The linear model

- It is rather cumbersome to try to get the correct least squares line, 

  - i. e. the line that minimizes the sum of squared residuals, 
  - through trial and error. 

Instead we can use the `lm` function in R 

  - to fit the linear model (a.k.a. regression line).

```{r}
m1 <- lm(runs ~ at_bats, data = mlb11)
```

The first argument in the function `lm` is a formula that takes the form `y ~ x`. 

  - Here it can be read that we want to make a linear model of runs 
    - as a function of at_bats. 
  - The second argument specifies that R should look in the mlb11 dataframe 
    - to find the runs and at_bats variables.

The output of `lm` is an object that contains all of the information we need 

  - about the linear model that was just fit. 
  - We can access this information using the summary function.

```{r} 
summary(m1)

## where is the sum of squares value here, how do iknow that what i chosewas a good fit
```

Let's consider this output piece by piece. 

  - First, the formula used to describe the model is shown at the top. 
  - After the formula you find the five-number summary of the residuals. 
  - The "Coefficients" table shown next is key; 
    - its first column displays the linear model's y-intercept 
    - and the coefficient of at_bats. 
  - With this table, we can write down 
    - the least squares regression line for the linear model:

$\hat{y}$ = -2789.2429 +  0.6305*$at_{bats}$

One last piece of information we will discuss from the summary output 

  - is the Multiple R-squared, or more simply, $R^2$. 
  - The $R^2$ value represents the proportion of variability 
  - in the response variable that is explained by the explanatory variable. 


  


#### LE6.3.3b ANSWER: 
0.3729 : 37 % variance of runs is explained by at_bats.

### LE6.3.4
 
- Fit a new model that uses homeruns to predict runs. 

  - Using the estimates from the R output, 
    - write the equation of the regression line. 
  - What does the slope tell us in the context of 
    - the relationship between success of a team and its home runs?

```{r}

## why don't we have the plot here
m2 <- lm(runs ~ homeruns, data = mlb11)
summary(m2)
```

#### LE6.3.4. ANSWER: 
- model m2 built above
-$\hat{y}$ = 415.2389 +  1.8345*$homeruns$
- slope : 1.8345, 
slops is positive implies, if homeruns increase, runs increase.



##### Prediction and prediction errors
 
- Create a scatterplot with the least squares line laid on top.
 
```{r}

## This was my next question
ggplot(mlb11,aes(homeruns, runs)) +
  geom_point() +
  geom_smooth(method='lm', se = FALSE)


```

The function `abline` plots a line based on its slope and intercept. 

  - How do you do it in `ggplot2`?

Here, we used a shortcut by providing the model m1, 

  - which contains both parameter estimates. 
  - This line can be used to predict y at any value of x. 
  - When predictions are made for values of x 
    - that are beyond the range of the observed data, 
    - it is referred to as extrapolation and is not usually recommended. 
  - However, predictions made within the range of the data are more reliable. 
  - They are also used to compute the residuals.
  

### LE6.3.5

- If a team manager saw the least squares regression line 

  - and not the actual data, 
  - how many runs would he or she predict for a team with 5,578 at-bats? 


#### LE6.3.5a  ANSWER:
725 : using the model line as refernece

```{r}

## see the line and answer the question.
ggplot(data = mlb11, aes(x = at_bats, y =runs)) +
  geom_point() +
  geom_smooth(method='lm', se = FALSE)

```


How many runs would he or she predict for a team with 5,578 at-bats? 

  - Is this an overestimate or an underestimate, and by how much? 
  - In other words, what is the residual for this prediction?

#### LE6.3.5b  ANSWER:
actual - 713 (from scatter plot), the residual is 725- 713  =  overestimate of 12

##### Model diagnostics

- To assess whether the linear model is reliable, 

  - we need to check for 
    - (1) linearity, 
    - (2) nearly normal residuals, and 
    - (3) constant variability.

Linearity: You already checked 

  - if the relationship between runs and at-bats is linear using a scatterplot. 
  - verify this condition with a plot of the residuals vs. at-bats. 

#### LE6.3.5c ANSWER: 
I alredy checked by plotting at_bats and runs in a scatter plot

- Plot of the residuals vs. at-bats. 

```{r}

## How do you plot residuals  ? How do we identify the points

## not usingggplot, because cannot get the data in a single df
plot(x = m1$model$at_bats, y = m1$residuals) 
# plot(x = m1$model$at_bats, y = m1$fitted.values)

mean(m1$residuals) ## the points average to 0


```



### LE6.3.6

- Is there any apparent pattern in the residuals plot? 

  - What does this indicate about the linearity of the relationship 
    - between runs and at-bats?

#### LE6.3.6 Answer: 
The residuals are centered around 0, that is because the best fit line equally
bisects the points



```{r}

```

### LE6.3.7

- Nearly normal residuals: To check this condition,

  - we can look at a histogram, 
    - `hist(m1$residuals)`, 
  - or a normal probability plot of the residuals.

```{r}
hist(m1$residuals)
```


Based on the histogram and the normal probability plot, 

  - does the nearly normal residuals condition appear to be met?

```{r} 

```

#### LE6.3.7 ANSWER:
YES, and the normal distribution is centered on 0

##### Constant variability:

### LE6.3.8

- Based on the plot above, 

  - does the constant variability condition appear to be met?

#### LE6.3.8 ANSWER

With the exception from the intervel 0-50, the constant variability condition 
seems to be met, the values at the two ends are the same, and they increase as we 
reach the center.
This means that there are only few residual points that lie away
from the fitted line and the points increase as we move towards the fitte line.

### LE6.3.9

- Choose another traditional variable from mlb11 

  - that you think might be a good predictor of runs. 

Produce a scatterplot of 

  - the two variables 
  - and fit a linear model. 

At a glance, does there seem to be a linear relationship?

#### LE6.3.9a ANSWER: 
Yes, there seems to be a linear relationship

```{r}
## choosing hits

mlb11 %>%
  ggplot(aes(x = hits, y= runs)) + geom_point()

ggplot(data = mlb11, aes(x = hits, y =runs)) +
  geom_point() +
  geom_smooth(method='lm', se = FALSE)

mdl_hits <- lm(runs ~ hits, data = mlb11)

## m1 r_squared value : 0.3729
summary(mdl_hits)
## mdl_hit r_squared_value : 0.6419




```


How does this relationship compare to the relationship between runs and at_bats? 

  - Use the $R^2$ values from the two model summaries to compare. 
  - Does your variable seem to predict runs 
    - better than at bats? 
  - How can you tell?

#### LE6.3.9b ANSWER: 

Hits : have a higher r squared_value than at_bats, it predicts the variability in
runs  almost double the times, than at_bats does.

r-squared value :  0.6419



### LE6.3.10

- Now that you can summarize the linear relationship between two variables, 

  - investigate the relationships 
    - between runs and each of the other five traditional variables. 

Which variable best predicts runs? 

Support your conclusion using 

  - the graphical 
  - and numerical methods we’ve discussed 
    - (for the sake of conciseness, 
    - only include output for the best variable, not all five).

#### LE6.3.10 ANSWER: 
bats_vg best predicts runs, with a r-squared of 0.6561
it has normal residuals, and at the ends the points are very low.
The data point that is maximum far away is at +40 and -40 respectively.


```{r}

### five variables are :
# already done for homeruns, at_bats and hits
# - batting average,
# – strikeouts,
# – stolen bases,
# – and wins.

# m1, m2, mdl_hits
## make a list of model objects

mdl_1 <- list(m1, m2 ,mdl_hits) # already created models

mdl_bat_av <- lm(runs ~ bat_avg, data = mlb11)
mdl_stk_outs <- lm(runs ~ strikeouts, data = mlb11)
mdl_stolen_bas <- lm(runs ~ stolen_bases, data = mlb11)
mdl_wins <- lm(runs ~ wins, data = mlb11)

mdl_2 <- list(mdl_bat_av, mdl_stk_outs, mdl_stolen_bas, mdl_wins)


mdl <- c(mdl_1, mdl_2 )# combining all models


# summary(mdl_bat_av)
sum_mdl <- list()
for (i in mdl){
  temp  <- as.list(summary(i))
  sum_mdl <- c(sum_mdl, temp )
}

for (i in 1:length(sum_mdl)) {
  print(sum_mdl[i]$call)
  print(sum_mdl[i]$r.squared)
  
  
}

## runs and bat average is the highest 0.6561 for mdl_bat_av

summary(mdl_bat_av)

ggplot(data = mlb11, aes(x = bat_avg, y =runs)) +
  geom_point() +
  geom_smooth(method='lm', se = FALSE)

hist(mdl_bat_av$residuals)
plot(x = mdl_bat_av$model$bat_avg, y = mdl_obs$residuals) 



```



### LE6.3.11

- Now examine the three newer variables. 

  - These are the statistics used by the author of Moneyball 
    - to predict a teams success. 
  - In general, are they more or less effective at predicting runs 
    - than the old variables? 
  - Explain using appropriate graphical and numerical evidence. 
  - Of all ten variables we’ve analyzed, 
    - which seems to be the best predictor of runs? 
  - Using the limited (or not so limited) information you know 
    - about these baseball statistics, 
    - does your result make sense?
  - Check the model diagnostics for the regression model 
    - with the variable you decided was the best predictor for runs.
  
#### LE6.3.11 ANSWER: 
They are more effective because of higher r squared values, all above 80%.
The best predictor of runs is new_obs variable.
Not knowing much about baseball, i cannot tell if this makes sense, but 
according to the data, newobs with the smalles r-squaredvalue is our best bet 
for finding runs.

  
```{r}

## three new variables:
# on-base percentage,
# • slugging percentage,
# • and on-base plus slugging
#"new_onbase"   "new_slug"     "new_obs"

mdl_onbase <- lm(runs ~ new_onbase, data = mlb11)
summary(mdl_onbase)
mdl_slug <- lm(runs ~ new_slug, data = mlb11)
summary(mdl_slug)
mdl_obs <- lm(runs ~ new_obs, data = mlb11)
summary(mdl_obs)


## 0.9349 is the highest _ new_obs


ggplot(data = mlb11, aes(x = new_obs, y =runs)) +
  geom_point() +
  geom_smooth(method='lm', se = FALSE)

hist(mdl_obs$residuals) # normally distributed constant variability
plot(x = mdl_obs$model$new_obs, y = mdl_obs$residuals) 


```
  
  
### LE6.3.12

- What concepts from the textbook are covered in this lab? 
 
   - What concepts, if any, are not covered in the textbook? 
   - Have you seen these concepts elsewhere, 
     - e.g. lecture, discussion section, previous labs, or homework problems? 
   - Be specific in your answer.
 
#### LE6.3.12 ANSWER: 
- Covered :
- Fitting a line, residuals, and correlation
- Least squares regression
- Extrapolation is treacherous

- Not covered :
-Inference for linear regression
-Types of outliers in linear regression
-----

## LE6.4 Timeseries Analysis

- Time series are a common type of data, 

  - consisting of measurements that are continuous over a time range. 

In this project we will be using classical decomposition 

  - to perform analysis on a time series.

First as an introduction to decomposition we will have a quick example.

### LE6.4.1

- What is the decomposition of a timeseries?

  - The AirPassengers data set of airline passengers every month for 12 years

```{r}
library(datasets)
library(tidyverse)
air <- as.data.frame(AirPassengers)
t <- theme(legend.text = element_text(size = 4) ) +
theme(legend.key.size = unit(0.5, 'cm') ,
axis.text.x = element_text(size = 6,
face = "bold", angle = 45, hjust = 1))

```

Plot the total time series of air passengers 

  - What do you notice about the plot?

#### LE6.4.1a ANSWER:
decomposition : breaking down time data, in a way so that we can see the various
periodicities , trends and noise separately.
The number of air passengers, keep on increasing as years pass.

```{r}


plot(AirPassengers)

```

Use the ts() function in base R 

  - to define AirPassengers as a time series with a yearly trend 

If the data is taken monthly, 

  - what will the frequency (points per season) of a yearly season be? 

#### LE6.4.1b ANSWER: 
12

```{r}

?ts()

#define AirPassengers as a time series with a yearly trend 
ap_ts <- ts(data = AirPassengers, frequency = 12)



```

Use the `decompose()` function 

  -to decompose the time series and remove the seasonality

The type for this time series is multiplicative

  - Plot the decomposed time series, 
  - what do you notice about the trend?

#### LE6.4.1c ANSWER: 
The trend of passengers keeps on increasing as the years pass.

```{r}

ap_ts_d <-decompose(ap_ts,type = "multiplicative")

plot(ap_ts_d$x, ylab="Air passengers raw",)
plot(ap_ts_d$seasonal, ylab="Seasonal decomp.")
plot(ap_ts_d$trend, ylab="Trend decomp.")

# The trend of passengers keeps on increasing as the years pass.


```

Isolate the trend and plot the trend on top of the raw data with the seasonality included

  - How well does the trend represent the data?

#### LE6.4.1d ANSWER: 
The trend mimicks a median line of the actual data, and therefore,
is able to capture, the major changes as the years pass, but does not
capture the variance within  a year.

```{r}

simulated <- (ap_ts_d$trend )
plot( ap_ts, type="l", col="red", ylab = "trend, raw" )
lines(simulated, col="green")



```

### LE6.4.2

- Now lets try this with a real world time series. 

  - In the LE6 data subfolder
  - `2108-351-351m-451-LE6-tsa-data-final.csv`

We'll be using one month of power and weather data from a solar power plant.

The data set variables are as follows:

  - `time`: The timestamp at which the data was taken
  - `ghir`: Global Horizontal Irradiance from a sensor at the site, 
    - the power from the sunlight 
    - over an area normal to the surface of the earth $(Watts/m^2)$
  - `iacp`: The AC power from the power plant $(kW)$
  - `temp`: The air temperature $(Celsius)$
  - `ghi_solargis`: The Global Horizontal Irradiance, not from a sensor, 
    - but predicted using weather modeling $(Watts/m^2)$
  - `clear`: A logical indicating 
    - whether the sky was "clear" during measurement, 
    - determined by comparing the ghi and ghi_solargis data
  - `ratio`: the ratio of the Global Horizontal Irradiance 
    - and the Plane of Array Irradiance 
    - (the irradiance normal to the surface of a tilted module)

The power from solar panels is dependent on the irradiance hitting it, 

  - so a power reading is often meaningless 
    - without a corresponding irradiance measurement.

It is useful to have multiple sources of irradiance measurements. 

Sensors on the ground are useful because 

  - they strongly represent the irradiance that is hitting the module; 
  - however, sensors can begin to drift if not cleaned or calibrated properly. 
  - An unstable sensor can render an entire data set useless.

To combat this, we also have irradiance data from SolarGIS, 

  - a company that uses satellite images to model and predict 
    - the irradiance at the surface of the earth. 

Plot the irradiance and power for the first week of data, 

  - how does the irradiance look compared to 
    - what you would expect from the trend of sunlight? 
  - How well does the power and irradiance match up? 

#### LE6.4.2a ANSWER: 
Irradiance in the sensor and the gis match well, as expected irradiance varies
every 24 hours or 1 day.
Power and irradiance match up well, as can be seen by the graph , where all 3 
are together.

```{r}
input1 <-read.csv("data/2108-351-351m-451-LE6-tsa-data-final.csv", header=TRUE)
head(input1)

## add labels to plots
## first week : first convert time to posicxt, then extract date and filter on it
input1 %>% mutate(
  date = format(
    as.POSIXct(time, format = "%Y-%m-%d %H:%M:%S"),format = "%Y-%m-%d")) %>%
  filter(date >= "2012-06-01" & date <= "2012-06-07") %>%
  ggplot(aes(x= as.POSIXct(time, format = "%Y-%m-%d %H:%M:%S"), y= ghir))+ geom_point() + xlab("date") + labs(title = "sensor_irradiance with time")+ t


input1 %>% mutate(
  date = format(
    as.POSIXct(time, format = "%Y-%m-%d %H:%M:%S"),format = "%Y-%m-%d")) %>%
  filter(date >= "2012-06-01" & date <= "2012-06-07") %>%
  ggplot(aes(x= as.POSIXct(time, format = "%Y-%m-%d %H:%M:%S"), y= iacp))+ geom_point() + xlab("date") + labs(title = "power with time") + t


input1 %>% mutate(
  date = format(
    as.POSIXct(time, format = "%Y-%m-%d %H:%M:%S"),format = "%Y-%m-%d")) %>%
  filter(date >= "2012-06-01" & date <= "2012-06-07") %>%
  ggplot(aes(x= as.POSIXct(time, format = "%Y-%m-%d %H:%M:%S"), y= ghi_solargis))+ geom_point() + xlab("date") + labs(title = "solargis with time") +t

input1 %>% mutate(
  date = format(
    as.POSIXct(time, format = "%Y-%m-%d %H:%M:%S"),format = "%Y-%m-%d")) %>%
  filter(date >= "2012-06-01" & date <= "2012-06-07") %>%
ggplot() +
geom_line(aes(x= as.POSIXct(time, format = "%Y-%m-%d %H:%M:%S"), y = ghir), group =1, color ="green")+
geom_line(aes(x= as.POSIXct(time, format = "%Y-%m-%d %H:%M:%S"), y = ghi_solargis), group =1, color ="blue")+
geom_line(aes(x= as.POSIXct(time, format = "%Y-%m-%d %H:%M:%S"), y = iacp), group =1, color ="red")+labs(title = "superposition of all pots with time") +
scale_y_continuous(sec.axis = sec_axis(trans =~./10, name ="iacp")) + xlab("date")+
  scale_colour_manual("",
                      breaks = c("iacp", "ghir", "ghi_solargis"),
                      values = c("iacp"="red", "ghir"="green", "ghi_solargis"="blue")) +t



  
  
  

```

Use the `ts()` functions and the `stlplus()` function from the `stlplus` package 

  - to decompse the sensor and SolarGIS irradiance and the power 
    - for the whole month. 
  -  Plot each of the decompositions, what do you notice?

#### LE6.4.2b ANSWER: 
Notice : all the different trends, seasonality and noise are similar.

```{r}
# think carefully about the frequency you'll need to define for this data
# what is the seasonal component to this data and how nay data points make up a season?
# use s.window = "periodic" for the stlplus function
library(stlplus)
?stlplus()

#convert data to timeseries objects

sen_ghi <- ts(data = input1$ghir, frequency =4*24) ## change this
gis_ghi <- ts(data = input1$ghi_solargis , frequency = 4*24)
pwr <- ts(data = input1$iacp, frequency = 4*24)

#decompose using stlplus
sg_d <- stlplus(sen_ghi,s.window = "periodic")
gg_d <- stlplus(gis_ghi,s.window = "periodic")
pwr_d <- stlplus(pwr,s.window = "periodic")

## do not know how to use ggplot, therefore reverting to NORMAL PLOT
# trend
plot(sg_d$data$trend, type="l", ylab = "sensor irradiance")
plot(gg_d$data$trend, type="l", ylab = "solargis irradiance")
plot(pwr_d$data$trend, type="l", ylab = "power irradiance")

# seasonality
plot(sg_d$data$seasonal, type="l", ylab = "sensor irradiance")
plot(gg_d$data$seasonal, type="l", ylab = "solargis irradiance")
plot(pwr_d$data$seasonal, type="l", ylab = "power irradiance")


# noise
plot(sg_d$data$remainder, type="l", ylab = "sensor irradiance")
plot(gg_d$data$remainder, type="l", ylab = "solargis irradiance")
plot(pwr_d$data$remainder, type="l", ylab = "power irradiance")




```


Isolate the trends for the 3 time series you just decomposed 

  - and build a linear model for each one.

Compare the models to each other, 

  - how are they different?

#### LE6.4.2c ANSWER: 
They have different slopes, but similar direction and spread.

```{r}

# plot(sg_d$data$trend, type="l")
# plot(gg_d$data$trend, type="l")
# plot(pwr_d$data$trend, type="l")


y <- sg_d$data$trend
x <- sg_d$time
model1 <- lm(y ~ x)
summary(model1)
plot(x,y, xlab="time", ylab="sensor irradiance", type = "l")

abline(model1, lwd =3, col ="red")


y <- gg_d$data$trend
x <- gg_d$time
model2 <- lm(y ~ x)
summary(model2)
plot(x,y, xlab="time", ylab="gis irradiance", type = "l")
abline(model2, lwd =3, col ="red")



y <- pwr_d$data$trend
x <- pwr_d$time
model3 <- lm(y ~ x)
summary(model3)
plot(x,y, xlab="time", ylab="pwr", type = "l")
abline(model3, lwd =3, col ="red")

# slope of each line is positive, with power being just a very small value 0.03276

```



Solar panel degradation leads to less power output over time 

  - at the same irradiance conditions.

Based on the linear models you found for the trends of power and irradiance, 

  - is this system degrading over time?

#### LE6.4.2d ANSWER:

The system is not degrading over time, because the slope is positive,0.03276.  


```{r}
y <- pwr_d$data$trend
x <- pwr_d$time
model3 <- lm(y ~ x)
summary(model3)
plot(x,y, xlab="time", ylab="pwr", type = "l")
abline(model3, lwd =3, col ="red")



```


How do the sensor GHI and the SolarGIS GHI compare to power?

#### LE6.4.2e ANSWER: 

sensor GHI has a lower slope than that of solarGHI, the sensors
could be hindered or be degrading if they do not reflect the true value, the power 
has a much smaller slope than either of these two values.
```{r}

y <- sg_d$data$trend
x <- sg_d$time
model1 <- lm(y ~ x)
summary(model1)
plot(x,y, xlab="time", ylab="sensor irradiance", type = "l")

abline(model1, lwd =3, col ="red")
# slope :1.43376

y <- gg_d$data$trend
x <- gg_d$time
model2 <- lm(y ~ x)
summary(model2)
plot(x,y, xlab="time", ylab="gis irradiance", type = "l")
abline(model2, lwd =3, col ="red")
#slope : 1.51993



```



#### Links

[http://www.r-project.org](http://www.r-project.org) 

[http://rmarkdown.rstudio.com/](http://rmarkdown.rstudio.com/)  

[https://www.openintro.org/stat/textbook.php?stat_book=os](https://www.openintro.org/stat/textbook.php?stat_book=os)